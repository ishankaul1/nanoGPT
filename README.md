# **NanoGPT SideQuest** âš”ï¸ðŸª„ðŸ°

Getting hands-on with **[NanoGPT](https://github.com/karpathy/nanoGPT)** to explore and understand:

- ðŸ§  Transformer architecture itself  
- ðŸ—ï¸ Infrastructure for training, evaluating, and serving transformers  
- ðŸ”¥ PyTorch fundamentals  
- ðŸ§ª The experimentation process end-to-end

---

## ðŸ—ºï¸ **Roadmap**  
> _Subject to change as tinkering evolves!_

### 1. ðŸš€ **Repeatable GPU Deployment**

Goal: Scripts, Dockerfiles, and infra to make it **easy to train / fine-tune + evaluate midâ€“large models** on GPUs.

**âœ… Checklist**

- [x] Single entrypoint scripts to prep data & train locally on Mac  
- [x] Helper scripts for deploying onto GPU provider (Runpod)  
- [ ] Flexible single-entrypoint Dockerfile deployable to RunPod  
  - _**You are here**_ ðŸ§­  
  - Most of this is set up â€” currently debugging why local run gets stuck in an infinite loop and never runs forward ðŸ¤”  
  - Example cmd:  
    ```bash
    ./build_and_run.sh run --no-gpu --device=cpu --max_iters=100 \
      --n_layer=2 --n_head=4 --n_embd=128 --batch_size=2 \
      --mount-output ./test_outputs
    ```
- [ ] Docker image successfully runs Shakespeare Char train on GPU  
- [ ] Checkpointed model, logs, and Weights & Biases (wandb) outputs can be pulled out cleanly  
- [ ] Full retrain on OpenWebText using GPU  
- [ ] Try distributed / faster runs  
- [ ] Fine-tune with Shakespeare or another dataset

---

### 2. ðŸ§  **Experimentation**

Some fun directions to try:

- âœ¨ Add **ROPE** (Rotary Position Embeddings)  
- ðŸ“ˆ Add more metrics to logging  
- ðŸ§® Tinker with model size & hyperparameters  
- ðŸ§  Incorporate more modern architectural add-ons (from newer papers)  
- ðŸª„ Experiment with fine-tuning using **QLoRA / LoRA** setups  
- ðŸ§­ Retrain or fine-tune using fancy RL methods

---

### 3. ðŸ§¬ **Advanced Stuff**  
> Might move out of this repo eventually.

- ðŸ§  Add **multimodality**  
- ðŸ§ª Fine-tune or modify **open-source models**

---

âœ¨ **Why this exists**: This is a sandbox for learning â€” less about shipping production code, more about **deeply understanding how transformers and their ecosystems work**.
